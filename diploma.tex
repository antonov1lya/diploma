\documentclass{article}
\usepackage{graphicx}

\usepackage[russian]{babel}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{hyperref}

\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newtheorem{theorem}{Теорема}[section]
\newtheorem{example}{Пример}[section]
\newtheorem{lemma}{Лемма}[section]

\def\ci{\perp\!\!\!\perp}
\usepackage{xcolor}

\begin{document}

\section{Введение}

Пусть $\{0,1\}^n = \{(x_1,\ldots,x_n): x_i \in \{0,1\} \}$.

\begin{definition}
    Случайный вектор $X=(X_1, X_2, X_3)^T$ имеет трехмерное распределение Бернулли,
    если множество его возможных значений $\{0,1\}^3$ и заданы вероятности:
    $$P(X_1=i,X_2=j,X_3=k)=p_{ijk} \geq 0, \quad \sum_{i=0}^{1}\sum_{j=0}^{1}\sum_{k=0}^{1}p_{ijk}=1$$
\end{definition}

\begin{definition}
    Пусть $X=(X_1,X_2,X_3)^T$ -- дискретный случайный вектор.
    Говорят, что случайные величины $X_1$ и $X_2$ условно независимы при условии $X_3$,
    и пишут $X_1 \ci X_2 \mid X_3$, если
    для любых $i,j$ и $k$, такого что $P(X_3=k)>0$, выполнено:
    $$
        P(X_1=i, X_2=j \mid X_3 = k) = P(X_1=i \mid X_3 = k) P(X_2=j \mid X_3 = k)
    $$
\end{definition}

\begin{theorem}\label{thm1}
    Пусть $(X_1,X_2,X_3)^T$ -- случайный вектор, имеющий трехмерное распределение Бернулли, и $P(X_3=0)>0$.
    Случайные величины $X_1$ и $X_2$ условно независимы при условии $X_3$ тогда и только тогда, когда
    $p_{00k}p_{11k}=p_{01k}p_{10k}$, где $k \in \{0,1\}$.
\end{theorem}

\begin{proof}
    Пусть $X_1 \ci X_2 \mid X_3$. Значит, для любых $(i,j,k) \in \{0,1\}^3$ выполнено условие:
    \begin{equation}\label{cond_ind}
        P(X_1=i, X_2=j \mid X_3 = k) = P(X_1=i \mid X_3 = k) P(X_2=j \mid X_3 = k)
    \end{equation}
    Найдем маргинальное распределение случайной величины $X_3$:
    $$
        P(X_3=k)=\sum_{i=0}^{1} \sum_{j=0}^{1} p_{ijk} = p_{00k} + p_{01k} + p_{10k} + p_{11k}
    $$
    Найдем маргинальные распределения $(X_1,X_3)^T$ и $(X_2,X_3)^T$:
    $$
        P(X_1=i, X_3=k) = \sum_{j=0}^{1} p_{ijk} = p_{i0k} + p_{i1k}
    $$
    $$
        P(X_2=j, X_3=k) = \sum_{i=0}^{1} p_{ijk} = p_{0jk} + p_{1jk}
    $$
    Запишем условные вероятности из условия \ref{cond_ind}:
    $$
        P(X_1=i, X_2=j \mid X_3 =k) = \dfrac{P(X_1=i, X_2=j , X_3 =k)}{P(X_3=k)} = \dfrac{p_{ijk}}{p_{00k} + p_{01k} + p_{10k} + p_{11k}}
    $$
    $$
        P(X_1=i \mid X_3 = k) = \dfrac{P(X_1=i, X_3 = k)}{P(X_3=k)} = \dfrac{p_{i0k} + p_{i1k}}{p_{00k} + p_{01k} + p_{10k} + p_{11k}}
    $$
    $$
        P(X_2=j \mid X_3 = k) = \dfrac{P(X_2=j, X_3=k)}{P(X_3=k)}=\dfrac{p_{0jk} + p_{1jk}}{p_{00k} + p_{01k} + p_{10k} + p_{11k}}
    $$
    Тогда условие \ref{cond_ind} можно переписать в следующем виде:
    $$
        \dfrac{p_{ijk}}{p_{00k} + p_{01k} + p_{10k} + p_{11k}} = \dfrac{p_{i0k} + p_{i1k}}{p_{00k} + p_{01k} + p_{10k} + p_{11k}}
        \dfrac{p_{0jk} + p_{1jk}}{p_{00k} + p_{01k} + p_{10k} + p_{11k}}
    $$
    $$
        p_{ijk} (p_{00k} + p_{01k} + p_{10k} + p_{11k}) = (p_{i0k} + p_{i1k}) (p_{0jk} + p_{1jk})
    $$
    Это условие выполняется для всех $(i,j,k) \in \{0,1\}^3$.
    Пусть $k \in \{0,1\}$ фиксировано.
    Если $i=0$ и $j=0$, то:
    $$
        p_{00k} (p_{00k} + p_{01k} + p_{10k} + p_{11k}) = (p_{00k} + p_{01k}) (p_{00k} + p_{10k})
    $$
    $$
        p_{00k} p_{00k} + p_{00k} p_{01k} + p_{00k} p_{10k} + p_{00k} p_{11k} =
        p_{00k} p_{00k} + p_{00k} p_{10k} + p_{01k} p_{00k} + p_{01k} p_{10k}
    $$
    $$
        p_{00k} p_{11k} = p_{01k} p_{10k}
    $$
    Если $i=0$ и $j=1$, то:
    $$
        p_{01k} (p_{00k} + p_{01k} + p_{10k} + p_{11k}) = (p_{00k} + p_{01k}) (p_{01k} + p_{11k})
    $$
    $$
        p_{01k}p_{00k} + p_{01k}p_{01k} + p_{01k}p_{10k} + p_{01k}p_{11k} =
        p_{00k}p_{01k} + p_{00k} p_{11k} + p_{01k} p_{01k} + p_{01k} p_{11k}
    $$
    $$
        p_{01k}p_{10k}=p_{00k} p_{11k}
    $$
    Если $i=1$ и $j=0$, то:
    $$
        p_{10k} (p_{00k} + p_{01k} + p_{10k} + p_{11k}) = (p_{10k} + p_{11k}) (p_{00k} + p_{10k})
    $$
    $$
        p_{10k} p_{00k} + p_{10k} p_{01k} + p_{10k} p_{10k} + p_{10k} p_{11k} = p_{10k}p_{00k} + p_{10k}p_{10k} + p_{11k}p_{00k} + p_{11k}p_{10k}
    $$
    $$
        p_{10k} p_{01k} = p_{11k}p_{00k}
    $$
    Если $i=1$ и $j=1$, то:
    $$
        p_{11k} (p_{00k} + p_{01k} + p_{10k} + p_{11k}) = (p_{10k} + p_{11k}) (p_{01k} + p_{11k})
    $$
    $$
        p_{11k} p_{00k} + p_{11k} p_{01k} + p_{11k} p_{10k} + p_{11k} p_{11k} = p_{10k} p_{01k} + p_{10k}p_{11k} + p_{11k}p_{01k} +
        p_{11k} p_{11k}
    $$
    $$
        p_{11k} p_{00k} = p_{10k} p_{01k}
    $$
    Таким образом, при $(i,j)\in \{0,1\}^2$ и фиксированном $k\in \{0,1\}$ из условия \ref{cond_ind} следует:
    $$p_{00k}p_{11k}=p_{01k}p_{10k}$$

    Поскольку в вышеприведенных рассуждениях все операции обратимые, то доказательство в обратную сторону проводится аналогично.
\end{proof}

\begin{example}
    Пусть $(X_1,X_2,X_3)$ имеет трехмерное распределение Бернулли с вероятностями
    $p_{000}=0.15$, $p_{001}=0.1$, $p_{010}=0.3$, $p_{011}=0.1$, $p_{100}=0.05$, $p_{101}=0.1$,
    $p_{110}=0.1$, $p_{111}=0.1$.
    Заметим, что:
    $$p_{000}p_{110}=p_{010}p_{100}=0.015$$ $$p_{001}p_{111}=p_{011}p_{101}=0.01$$
    Значит из теоремы \ref{thm1} следует, что $X_1 \ci X_2 \mid X_3$.
\end{example}

\section{Частный коэффициент корреляции Пирсона}
Для удобства введем следующие обозначения: $$p_{i**}=P(X_1=i), \quad p_{*j*}=P(X_2=j), \quad p_{**k}=P(X_3=k)$$
$$p_{ij*}=P(X_1=i, X_2=j), \quad p_{i*k}=P(X_1=i, X_3=k), \quad p_{*jk}=P(X_2=j, X_3=k)$$
Далее символом $\Sigma$ будем обозначать ковариационную матрицу:
$$\Sigma =
    \begin{pmatrix}
        \sigma_{11} & \sigma_{12} & \sigma_{13} \\
        \sigma_{21} & \sigma_{22} & \sigma_{23} \\
        \sigma_{31} & \sigma_{32} & \sigma_{33}
    \end{pmatrix}
$$
Легко проверить, что $\sigma_{11}=D(X_1) = p_{1**}(1-p_{1**})$.

\begin{lemma}
    $$\sigma_{12}=\text{Cov}(X_1,X_2)=p_{11*}-p_{1**}p_{*1*}$$
\end{lemma}

\begin{proof}
    Воспользуемся формулой $\text{Cov}(X_1,X_2)=E(X_1 X_2)-E(X_1)E(X_2)$.
    $$E(X_1 X_2) = 1 \cdot p_{11*} + 0 \cdot (p_{00*} + p_{01*} + p_{10*})=p_{11*}$$
    $$EX_1 = 1 \cdot p_{1**} + 0 \cdot p_{0**}=p_{1**}$$
    $$ EX_2 = 1 \cdot p_{*1*} + 0 \cdot p_{*0*} = p_{*1*}$$
    Таким образом, $\text{Cov}(X_1,X_2)=p_{11*}-p_{1**}p_{*1*}$.
\end{proof}

% Сформулируем следующую теорему.
% \begin{theorem}
%     Случайные величины $X_1$ и $X_2$ независимы тогда и только тогда, когда $\sigma_{12}=0$.
% \end{theorem}
% \begin{proof}
%     Пусть случайные величины $X_1$ и $X_2$ независимы. Тогда выполнено условие:
%     $$
%     P(X_1=1,X_2=1)=P(X_1=1)P(X_2=1)
%     $$
%     Перепишем его в новых обозначениях и преобразуем:
%     $$
%     p_{11*}-p_{1**}p_{*1*}=0
%     $$
%     Из чего следует, что $\sigma_{12}=0$.

%     Пусть $\sigma_{12}=0$, то есть $p_{11*}-p_{1**}p_{*1*}=0$. Другими словами: $$P(X_1=1, X_2=1)=P(X_1=1)P(X_2=1)$$
%     Из чего следует, что $X_1$ и $X_2$ независимы (поскольку независимость событий $A$ и $B$ эквивалетна независимости событий
%     $\overline{A}$ и $B$, $A$ и $\overline{B}$, $\overline{A}$ и $\overline{B}$).
% \end{proof}

Частный коэффициент корреляции Пирсона определяется через элементы обратной ковариационной матрицы $\Sigma^{-1}$:
$$
    \Sigma^{-1}=\begin{pmatrix}
        \sigma^{11} & \sigma^{12} & \sigma^{13} \\
        \sigma^{21} & \sigma^{22} & \sigma^{23} \\
        \sigma^{31} & \sigma^{32} & \sigma^{33}
    \end{pmatrix}
$$
Из линейной алгебры известно, что элемент $\sigma^{12}$ матрицы $\Sigma^{-1}$ выражается через соотношение
$\sigma^{12}=\dfrac{(-1)^{2+1}}{\text{det} (\Sigma)}M_{21}$, где
$$
    M_{21}=\text{det}
    \begin{pmatrix}
        \sigma_{12} & \sigma_{13} \\
        \sigma_{32} & \sigma_{33}
    \end{pmatrix}
$$

\begin{lemma}\label{partial_cov}
    $$M_{21} = p_{**0}(p_{001}p_{111}-p_{011}p_{101}) + p_{**1} (p_{000}p_{110}-p_{010}p_{100})$$
\end{lemma}
\begin{proof}
    $$ M_{21}= \text{det}
        \begin{pmatrix}
            \sigma_{12} & \sigma_{13} \\
            \sigma_{23} & \sigma_{33}
        \end{pmatrix}
        = (p_{11*}-p_{1**}p_{*1*}) p_{**1}(1-p_{**1})-(p_{1*1}-p_{1**}p_{**1})(p_{*11}-p_{*1*}p_{**1})=
    $$
    Раскроем скобки:
    $$
        =p_{11*}p_{**1} - p_{11*}p_{**1}p_{**1} - p_{1**}p_{*1*}p_{**1} + p_{1**}p_{*1*}p_{**1}p_{**1}-
    $$
    $$
        -p_{1*1}p_{*11}+p_{1*1}p_{*1*}p_{**1}+p_{1**}p_{**1}p_{*11}-p_{1**}p_{**1}p_{*1*}p_{**1}=
    $$
    Сократим 4-е и 8-е слагаемые. Распишем 1-е слагаемое как сумму вероятностей:
    $$
        =(p_{111}p_{**1}+p_{110}p_{**1}) - p_{11*}p_{**1}p_{**1} - p_{1**}p_{*1*}p_{**1} -p_{1*1}p_{*11}+p_{1*1}p_{*1*}p_{**1}+p_{1**}p_{**1}p_{*11}=
    $$
    Перегруппируем слагаемые.
    $$
        =(p_{111}p_{**1}-p_{1*1}p_{*11})+p_{**1}(p_{110}-p_{11*}p_{**1} - p_{1**}p_{*1*} + p_{1*1}p_{*1*} + p_{1**}p_{*11})
    $$
    Заметим, что:
    $$
        p_{110}-p_{11*}p_{**1}=p_{110}-p_{110}p_{**1}-p_{111}p_{**1}=p_{110}(1-p_{**1})-p_{111}p_{**1}=p_{110}p_{**0}-p_{111}p_{**1}
    $$
    Также заметим, что:
    $$
        -p_{1**}p_{*1*} + p_{1*1}p_{*1*} + p_{1**}p_{*11}=
    $$
    $$
        =-(p_{1*0}+p_{1*1})(p_{*10}+p_{*11})+p_{1*1}(p_{*10}+p_{*11}) + (p_{1*0}+p_{1*1})p_{*11}=
    $$
    $$
        =-p_{1*0}p_{*10}-p_{1*0}p_{*11}-p_{1*1}p_{*10}-p_{1*1}p_{*11}+p_{1*1}p_{*10}+p_{1*1}p_{*11}+p_{1*0}p_{*11}+p_{1*1}p_{*11}=
    $$
    $$
        =-p_{1*0}p_{*10}+p_{1*1}p_{*11}
    $$
    Запишем выражение для $M_{21}$:
    $$
        M_{21}=(p_{111}p_{**1}-p_{1*1}p_{*11})+p_{**1}((p_{110}p_{**0}-p_{1*0}p_{*10})-(p_{111}p_{**1}-p_{1*1}p_{*11}))=
    $$
    $$
        =(p_{111}p_{**1}-p_{1*1}p_{*11})+p_{**1}(p_{110}p_{**0}-p_{1*0}p_{*10})-p_{**1}(p_{111}p_{**1}-p_{1*1}p_{*11})=
    $$
    $$
        =(1-p_{**1})(p_{111}p_{**1}-p_{1*1}p_{*11})+p_{**1}(p_{110}p_{**0}-p_{1*0}p_{*10})=
    $$
    $$
        =p_{**0}(p_{111}p_{**1}-p_{1*1}p_{*11})+p_{**1}(p_{110}p_{**0}-p_{1*0}p_{*10})
    $$
    Преобразуем выражение:
    $$
        p_{111}p_{**1}-p_{1*1}p_{*11} = p_{111}(p_{001}+p_{011}+p_{101}+p_{111})-
        (p_{101}+p_{111})(p_{011}+p_{111})=
    $$
    $$
        = p_{111}p_{001}+p_{111}p_{011}+p_{111}p_{101}+p_{111}p_{111}
        - p_{101}p_{011}-p_{101}p_{111}-p_{111}p_{011}-p_{111}p_{111}=
    $$
    $$
        = p_{001}p_{111}-p_{011}p_{101}
    $$
    Аналогично преобразуем выражение:
    $$
        p_{110}p_{**0}-p_{1*0}p_{*10}=
        p_{110}(p_{000}+p_{010}+p_{100}+p_{110})-(p_{100}+p_{110})(p_{010}+p_{110})=
    $$
    $$
        =p_{110}p_{000}+p_{110}p_{010}+p_{110}p_{100}+p_{110}p_{110}
        -p_{100}p_{010}-p_{100}p_{110}-p_{110}p_{010}-p_{110}p_{110}=
    $$
    $$
        =p_{000}p_{110}-p_{010}p_{100}
    $$
    Таким образом:
    $$
        M_{21} = p_{**0}(p_{001}p_{111}-p_{011}p_{101}) + p_{**1} (p_{000}p_{110}-p_{010}p_{100})
    $$
\end{proof}
\begin{theorem}\label{1.2}
    Пусть $X_1$ и $X_2$ условно независимы при условии $X_3$. Тогда $\sigma^{12}=0$.
\end{theorem}
\begin{proof}
    Пусть $X_1$ и $X_2$ условно независимы при условии $X_3$. Тогда по теореме \ref{thm1}:
    $$p_{000}p_{110}=p_{010}p_{100}$$
    $$p_{001}p_{111}=p_{011}p_{101}$$
    Используя вышеприведенные соотношения, имеем:
    $$
        M_{21} = p_{**0}(p_{001}p_{111}-p_{011}p_{101}) + p_{**1} (p_{000}p_{110}-p_{010}p_{100})=p_{**0}\cdot 0 + p_{**1} \cdot 0 = 0
    $$
    Из $M_{21}=0$ непосредственно следует, что $\sigma^{12}=0$.
\end{proof}
В обратную сторону теорема \ref{1.2} неверна. Легко построить контрпример при $p_{**0}=0$. Мы же покажем контрпример в невырожденном случае.
\begin{example}
    Пусть $p_{000}=0.15$, $p_{001}=0.1$, $p_{010}=0.1$, $p_{011}=0.15$, $p_{100}=0.1$, $p_{101}=0.15$, $p_{110}=0.15$, $p_{111}=0.1$.
    Тогда $p_{**0}=0.5$, $p_{**1}=0.5$ и

    $$M_{21} = p_{**1}(p_{000}p_{110}-p_{010}p_{100}) + p_{**0}(p_{001}p_{111}-p_{011}p_{101})=$$
    $$= 0.5 \cdot (0.15 \cdot 0.15 - 0.1 \cdot 0.1) + 0.5 \cdot (0.1 \cdot 0.1 - 0.15 \cdot 0.15) = 0$$

    Однако, случайные величины $X_1$ и $X_2$ условно зависимы при условии $X_3$ поскольку:
    $$
        p_{000}p_{110}-p_{010}p_{100}=0.15 \cdot 0.15 - 0.1 \cdot 0.1 = 0.0125 \neq 0
    $$
    $$
        p_{001}p_{111}-p_{011}p_{101}=0.1 \cdot 0.1 - 0.15 \cdot 0.15 = -0.0125 \neq 0
    $$

\end{example}
\end{document}
