\begin{center}
    \subsection{РНМН тест проверки независимости в условном
распределении}\label{bivariate_umpu}
\end{center}
     
% В следующем разделе будет показано, что проверку 
% условной независимости в трехмерном распределении Бернулли можно 
% свести к множественной проверке независимости в условных распределениях.
% Согласно работе \cite{Dai2013}, случайный вектор с 
% трехмерным распределением Бернулли
% при одной фиксированной компоненте имеет
% двумерное распределение Бернулли. Приведем теорию проверки независимости
% в двумерном распределении Бернулли.

Пусть $(X,Y,Z)^T$ -- случайный вектор, имеющий трехмерное распределение Бернулли.
При фиксированном $Z=z$:
$$
P(X=x,Y=y \mid Z=z) =
\dfrac{p_{xyz}}{p_{00z}+p_{01z}+p_{10z}+p_{11z}} = q_{xy}
$$
Приведем определение случайного вектора с двумерным распределением Бернулли.
\begin{definition}
    Случайный вектор $(X,Y)^T$ имеет двумерное распределение Бернулли,
    если множество его возможных значений:
    $$
        \begin{pmatrix}
            0 \\
            0
        \end{pmatrix},
        \begin{pmatrix}
            0 \\
            1
        \end{pmatrix},
        \begin{pmatrix}
            1 \\
            0
        \end{pmatrix},
        \begin{pmatrix}
            1 \\
            1
        \end{pmatrix}
    $$ и заданы $P(X=x,Y=y)=q_{xy} \geq 0,  \sum_{x=0}^1 \sum_{y=0}^1 q_{xy} =1$.
\end{definition}
Таким образом, $(X,Y)^T$ при условии $Z=z$ имеет двумерное распределении Бернулли.
Изложим теорию проверки независимости в двумерном распределении Бернулли,
которая в \autoref{twos} будет использована для проверки условной независимости.

В работе \cite{Dai2013} показано, что:
$$
P(X=x,Y=y)= \exp \left\{\ln(q_{00}) + 
     \ln \left(\dfrac{q_{10}}{q_{00}}\right) x
    +  \ln \left(\dfrac{q_{01}}{q_{00}}\right) y +
     \ln\left(\dfrac{q_{00}q_{11}}{q_{01}q_{10}}\right) xy
 \right\}
$$
Также, в \cite{Dai2013} доказано, что 
$X$ и $Y$ независимы тогда и только тогда, когда
$$\theta=\ln\left(\dfrac{q_{00}q_{11}}{q_{01}q_{10}}\right)=0$$
Пусть
    $$
        \begin{pmatrix}
            X_1 \\
            Y_1 
        \end{pmatrix},
        \begin{pmatrix}
            X_2 \\
            Y_2
        \end{pmatrix}, \ldots,
        \begin{pmatrix}
            X_n \\
            Y_n
        \end{pmatrix}
    $$ повторная выборка из распределения случайного вектора $(X,Y)^T$. Тогда:
$$
    P(X_1=x_1,Y_1=y_1,\ldots,X_n=x_n,Y_n=y_n)
    =\prod_{i=1}^n P(X_i=x_i,Y_i=y_i) =
    $$
    $$
    =\exp \left\{ \ln(q_{00})n + 
        \ln \left(\dfrac{q_{10}}{q_{00}}\right) \sum_{i=1}^n x_i 
        +\ln \left(\dfrac{q_{01}}{q_{00}}\right) \sum_{i=1}^{n} y_i   +
        \ln\left(\dfrac{q_{00}q_{11}}{q_{01}q_{10}}\right) \sum_{i=1}^n x_i y_i 
     \right\}
    $$
    Пусть 
    $$
    U = \sum_{i=1}^n X_i Y_i,\;
    T_1 = \sum_{i=1}^n X_i,\;
    T_2 = \sum_{i=1}^n Y_i
    $$
    Обозначим $T=(T_1,T_2)$, $t=(t_1,t_2)$, $\theta_0=0$.
    Тогда согласно \cite{Lehmann1986} РНМН тест проверки гипотезы $h^{\text{Independence}}: \theta = 0$ против альтернативы $k^{\text{Independence}}: \theta \neq 0$ 
    уровня $\alpha$ имеет вид:
    $$
    \varphi^{\text{Independence}}(u,t)=\begin{cases}
        1, \; u<C_1(t) \text{ или } u>C_2(t)\\
        \gamma_i, \; u=C_i(t), \; i=1,2\\
        0, \; C_1(t)<u<C_2(t)
    \end{cases}
    $$
    где константы $C_i$ и $\gamma_i$ определяются из системы уравнений:
    $$
    \begin{cases}
        E_{\theta_0}[\varphi^{\text{Independence}}(U,T) \mid T=t]=\alpha \\
        E_{\theta_0}[U\varphi^{\text{Independence}}(U,T) \mid T=t]=\alpha E_{\theta_0}[U \mid T=t]
    \end{cases}
    $$
    Приведем распределение статистики $U$ при условии $T=t$.
\begin{lemma}
    Пусть $k_1(u)=u$, $k_2(u)=t_1-u$, $k_3(u)=t_2-u$,
    $k_4(u)=n-t_1-t_2+u$.
        Тогда
        $$P_{\theta_0}(U=u \mid T=t)=\dfrac{(\prod_{i=1}^4 k_i(u)!)^{-1}}
            {\sum_{s\in \mathcal{D}} (\prod_{i=1}^4 k_i(s)!)^{-1}}$$
        где $\mathcal{D}=\{s \in \mathbb{Z}: 0\leq k_i(s) \leq n \text{ для всех } i=1\ldots,4\}$.
\end{lemma}
Доказательство данной леммы не приводится, поскольку оно полностью 
аналогично доказательству леммы \ref{u_dist}.

Стоит отметить, что
$
\ln\left(\dfrac{q_{00}q_{11}}{q_{01}q_{10}}\right) =
\ln\left(\dfrac{p_{00z}p_{11z}}{p_{01z}p_{10z}}\right)
$. Поэтому проверяя гипотезу о параметре
$\ln\left(\dfrac{q_{00}q_{11}}{q_{01}q_{10}}\right)$ в условном распределении,
мы проверяем гипотезу о параметре $\ln\left(\dfrac{p_{00z}p_{11z}}{p_{01z}p_{10z}}\right)$
в трехмерном распределении Бернулли.

% \begin{proof}
%     $$P(U=u,T_1=t_1,T_2=t_2)=
%     P\left(\sum_{i=1}^n X_i Y_i=u,\sum_{i=1}^n X_i=t_1,
%     \sum_{i=1}^n Y_i=t_2\right)=$$
%     $$=P\biggl(
%         \sum_{i=1}^n X_i Y_i=u, \sum_{i=1}^n X_i (1-Y_i)=t_1-u,
%         \sum_{i=1}^n (1-X_i) Y_i=t_2-u,$$
%         $$
%         \sum_{i=1}^n (1-X_i) (1-Y_i)=n-t_1-t_2+u
%         \biggr)
%     =\dfrac{n!}{\prod_{i=1}^{4}k_i(u)!}
%     p_{11}^u p_{10}^{t_1-u} p_{01}^{t_2-u} p_{00}^{n-t_1-t_2+u}
%     $$
%     $$
%     P(U=u \mid T_1=t_1, T_2=t_2)=
%     \dfrac{(\prod_{i=1}^{4}k_i(u)!)^{-1}
%     \left(\dfrac{p_{00}p_{11}}{p_{01}p_{10}}\right)^u}
%     {\sum_{s}(\prod_{i=1}^{4}k_i(s)!)^{-1}
%     \left(\dfrac{p_{00}p_{11}}{p_{01}p_{10}}\right)^s}
%     $$
%     При истинности гипотезы $\theta=0$ параметр 
%     $\dfrac{p_{00}p_{11}}{p_{01}p_{10}}=1$. Тогда:
%     $$
%     P_{\theta=0}(U=u \mid T_1=t_1, T_2=t_2)=
%     \dfrac{(\prod_{i=1}^{4}k_i(u)!)^{-1}}
%     {\sum_{s}(\prod_{i=1}^{4}k_i(s)!)^{-1}}
%     $$
% \end{proof}