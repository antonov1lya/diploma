\begin{centering}
    \subsection{Проверка условной независимости по подвыборкам из условных распределений}
\end{centering}

Приведем трактовку определения \ref{cond_ind_def} для трехмерного
распределения Бернулли. Случайные величины $X$ и $Y$
условно независимы при условии $Z$ тогда и только тогда,
когда:
\begin{itemize}
    \item $X$ и $Y$ независимы при условии $Z=0$
    \item $X$ и $Y$  независимы при условии $Z=1$
\end{itemize}
Это порождает следующий способ проверки условной независимости. 
Сформулируем индивидуальные гипотезы:
\begin{itemize}
    \item $h_0 : X$ и $Y$ независимы при условии $Z=0$
    \item $h_1 : X$ и $Y$ независимы при условии $Z=1$
\end{itemize}
Тогда гипотеза об условной независимости имеет вид 
$h = h_0 \cap h_1$. Естественным образом, гипотезу 
$h_0$ необходимо проверять по наблюдениям
$(x_i,y_i,z_i)^T$, в которых $z_i=0$. Поскольку $(X,Y)^T$ при условии
$Z=z$ имеет двумерное распределение Бернулли \cite{Dai2013}, то
в качестве теста для $h_0$ можно использовать
$\varphi_0 = \varphi^{\text{Independence}}_0$, 
приведенный в разделе \ref{bivariate_umpu}. Аналогичные рассуждения 
справедливы и для гипотезы $h_1$. Учитывая озвученные соображения,
построим тест проверки гипотезы $h$, контролирующий вероятность
ошибки первого рода на уровне $\alpha$.
Пусть
$$
\begin{pmatrix}
        X_1 \\
        Y_1 \\
        Z_1
    \end{pmatrix},
    \begin{pmatrix}
        X_2 \\
        Y_2 \\
        Z_2
    \end{pmatrix}, \ldots,
    \begin{pmatrix}
        X_n \\
        Y_n \\
        Z_n
    \end{pmatrix}
$$ повторная выборка из распределения случайного вектора $(X,Y,Z)^T$. Обозначим $\mathbf{Z}=(Z_1,\ldots,Z_n)$ и 
$\mathbf{z}=(z_1,\ldots,z_n)$.
Покажем, что в условном распределении при $\mathbf{Z}=\mathbf{z}$ наблюдения:
$$
\Xi = 
\begin{pmatrix}
        X_1 \\
        Y_1
    \end{pmatrix},
    \begin{pmatrix}
        X_2 \\
        Y_2
    \end{pmatrix}, \ldots,
    \begin{pmatrix}
        X_n \\
        Y_n
    \end{pmatrix}
$$
являются независимыми. 

\begin{lemma}\label{ci_for_samples}
    $$
    P(X_1=x_1,Y_1=y_1,\ldots,X_n=x_n,Y_n=y_n \mid \mathbf{Z}=\mathbf{z})=
    $$
    $$
    =\prod_{i=1}^n P(X_i=x_i, Y_i=y_i \mid \mathbf{Z}=\mathbf{z})
    $$
\end{lemma}
\begin{proof}
    С одной стороны:
    $$
    P(X_1=x_1,Y_1=y_1,Z_1=z_1,\ldots,X_n=x_n,Y_n=y_n,Z_n=z_n)=
    $$
    $$
    =P(\mathbf{Z}=\mathbf{z}) P(X_1=x_1,Y_1=y_1,\ldots,X_n=x_n,Y_n=y_n \mid \mathbf{Z}=\mathbf{z}) 
    $$
    С другой стороны:
    $$
    P(X_1=x_1,Y_1=y_1,Z_1=z_1,\ldots,X_n=x_n,Y_n=y_n,Z_n=z_n)=
    $$
    $$
    =\prod_{i=1}^n P(X_i=x_i, Y_i=y_i, Z_i=z_i)=
    $$
    $$
    =\prod_{i=1}^n P(X_i=x_i, Y_i=y_i \mid Z_i=z_i)P(Z_i=z_i)=
    $$
    $$
    =P(\mathbf{Z}=\mathbf{z})\prod_{i=1}^n P(X_i=x_i, Y_i=y_i \mid \mathbf{Z}=\mathbf{z})
    $$
\end{proof}

Пусть также $\mathbf{Z}=\mathbf{z}$ фиксированы. 
Разобьем выборку $\Xi$ на две подвыборки $\Xi_0$ и $\Xi_1$, 
такие что:
$$
\Xi_0=
\begin{pmatrix}
    X_{i_1} \\
    Y_{i_1} \\
\end{pmatrix},
\begin{pmatrix}
    X_{i_2} \\
    Y_{i_2} \\
\end{pmatrix}, \ldots,
\begin{pmatrix}
    X_{i_{n_0}} \\
    Y_{i_{n_0}} \\
\end{pmatrix} 
$$
где $Z_{i_k}=z_{i_k}=0$ для всех $i_k$ при $k=\overline{1,n_0}$ и $$
\Xi_1=
\begin{pmatrix}
    X_{j_1} \\
    Y_{j_1} \\
\end{pmatrix},
\begin{pmatrix}
    X_{j_2} \\
    Y_{j_2} \\
\end{pmatrix}, \ldots,
\begin{pmatrix}
    X_{j_{n_1}} \\
    Y_{j_{n_1}} \\
\end{pmatrix} 
$$
где $Z_{j_k}=z_{j_k}=1$ для всех $j_k$ при $k=\overline{1,n_1}$. 
Причем $n=n_0+n_1$. Отметим, что
разбиение $\Xi = \Xi_0 \sqcup \Xi_1$ опреляется лишь набором
$\mathbf{Z}=\mathbf{z}$.

Сформулируем следующую теорему.

\begin{theorem}
    Пусть $\mathbf{Z}=\mathbf{z}$ -- фиксированы. Тогда выборка $\Xi_0$ является повторной выборкой из распределения $(X,Y)^T$ при условии $Z=0$.
\end{theorem}
\begin{proof}
    По лемме \ref{ci_for_samples}:
    $$
    P(X_1=x_1,Y_1=y_1,\ldots,X_n=x_n,Y_n=y_n \mid \mathbf{Z}=\mathbf{z})=
    $$
    $$
    =\prod_{i=1}^n P(X_i=x_i, Y_i=y_i \mid \mathbf{Z}=\mathbf{z})
    $$
    Просуммировав обе части вышепредставленного равенства по всем возможным значениям 
    $x_{j_1}, y_{j_1}, \ldots, x_{j_{n_1}},y_{j_{n_1}}$ получаем:
    $$
    P(X_{i_1}=x_{i_1},Y_{i_1}=y_{i_1},\ldots,X_{i_{n_0}}=x_{i_{n_0}},Y_{i_{n_0}}=y_{i_{n_0}} \mid \mathbf{Z}=\mathbf{z})=
    $$
    $$
    =\prod_{k=1}^{n_0} P(X_{i_k}=x_{i_k}, Y_{i_k}=y_{i_k} \mid \mathbf{Z}=\mathbf{z})
    $$
    Значит $$
    \Xi_0=
    \begin{pmatrix}
        X_{i_1} \\
        Y_{i_1} \\
    \end{pmatrix},
    \begin{pmatrix}
        X_{i_2} \\
        Y_{i_2} \\
    \end{pmatrix}, \ldots,
    \begin{pmatrix}
        X_{i_{n_0}} \\
        Y_{i_{n_0}} \\
    \end{pmatrix} 
    $$
    независимые наблюдения при условии $\mathbf{Z}=\mathbf{z}$.

    Покажем, что $(X_{i_k},Y_{i_k})^T$ при условии $\mathbf{Z}=\mathbf{z}$ распределен также как и 
    $(X,Y)^T$ при условии $Z=0$.
    $$
    P(X_{i_k}=x_{i_k}, Y_{i_k}=y_{i_k} \mid \mathbf{Z}=\mathbf{z}) =
    P(X_{i_k}=x_{i_k}, Y_{i_k}=y_{i_k} \mid Z_{i_k}=z_{i_k}) = $$
    $$=P(X_{i_k}=x_{i_k}, Y_{i_k}=y_{i_k} \mid Z_{i_{n_k}}=0)=
    P(X=x_{i_k}, Y=y_{i_k} \mid Z=0)$$
\end{proof}

Аналогично показывается, что $\Xi_1$ является повторной выборкой из распределения $(X,Y)^T$ при условии $Z=1$.

Для упрощения записи будем использовать нотацию 
$P(A \mid B)=P_{B}(A)$. Сформулируем теорему.
\begin{theorem}\label{main_theorem}
    Пусть $\Xi_0$ и $\Xi_1$ -- подвыборки, полученные разбиением случайной выборки $\Xi$.
    Пусть $\varphi_0$ и $\varphi_1$ -- рандомизированные тесты 
    проверки гипотез $h_0$ и $h_1$ по повторным выборкам 
    $\Xi_0$ и $\Xi_1$ соответственно.
    Введем события:
    $$A_0 = \{\text{отвергнуть гипотезу $h_0$ рандомизированным тестом $\varphi_0$}\}$$ 
    $$A_1 = \{\text{отвергнуть гипотезу $h_1$ рандомизированным тестом $\varphi_1$}\}$$
    Пусть $\varphi_0$ и $\varphi_1$ тесты уровня $\alpha_0$ и $\alpha_1$
    при любом объеме наблюдений в подвыборках $\Xi_0$ и $\Xi_1$ соответственно, то есть:
    $$P_{h_0\cap h_1,\mathbf{Z=z}}(A_0)=\alpha_0$$ 
    $$P_{h_0\cap h_1,\mathbf{Z=z}}(A_1)=\alpha_1$$
    Тогда $P_{h_0\cap h_1}(A_0 \cap A_1)= P_{h_0\cap h_1}(A_0) P_{h_0\cap h_1}(A_1)$.
\end{theorem}
\begin{proof}
    Пусть $\mathbf{Z}=\mathbf{z}$ фиксировано. 
    Обозначим через
    $$T_0=(X_{i_1},Y_{i_1},\ldots,X_{i_{n_0}},Y_{i_{n_0}})^T, \;
    T_1=(X_{j_1},Y_{j_1},\ldots,X_{j_{n_1}},Y_{j_{n_1}})^T$$
    случайные векторы наблюдений, используемые в тестах $\varphi_0$ и 
    $\varphi_1$ соответственно.
    Распишем следующую вероятность:
    $$
    P_{h_0\cap h_1,\mathbf{Z=z}}(A_0 \cap A_1)=
    $$
    $$
    =\sum_{t_0}\sum_{t_1} P_{h_0\cap h_1,\mathbf{Z=z}}(A_0 \cap A_1 \mid T_0=t_0, T_1=t_1)P_{h_0\cap h_1,\mathbf{Z=z}}(T_0=t_0, T_1=t_1)
    $$
    Отметим, что $P_{h_0\cap h_1,\mathbf{Z=z}}(A_0 \cap A_1 \mid T_0=t_0, T_1=t_1)=\varphi_0(t_0)\varphi_1(t_1)$, поскольку
    для того, чтобы при известных значениях статистик $t_0$ и $t_1$ отвергнуть гипотезы $h_0$ и $h_1$ в рандомизированном тесте нужно провести два испытания с вероятностью успеха
    $\varphi_0(t_0)$ и $\varphi_1(t_1)$. Постулируется, что такие испытания независимые. Тогда:
    $$
    P_{h_0\cap h_1,\mathbf{Z=z}}(A_0 \cap A_1)=\sum_{t_0}\sum_{t_1} \varphi_0(t_0) \varphi_1(t_1) P_{h_0\cap h_1,\mathbf{Z=z}}(T_0=t_0, T_1=t_1)=
    $$
    $$
    =\sum_{t_0}\sum_{t_1} \varphi_0(t_0) \varphi_1(t_1) P_{h_0\cap h_1,\mathbf{Z=z}}(T_0=t_0)P_{h_0\cap h_1,\mathbf{Z=z}}(T_1=t_1)=
    $$
    $$
    =\sum_{t_0}\left[ \varphi_0(t_0) P_{h_0\cap h_1,\mathbf{Z=z}}(T_0=t_0) \left(\sum_{t_1}\varphi_1(t_1) P_{h_0\cap h_1,\mathbf{Z=z}}(T_1=t_1)\right)\right]=
    $$
    $$
    = \sum_{t_0} \varphi_0(t_0) P_{h_0\cap h_1,\mathbf{Z=z}}(T_0=t_0) \alpha_1 
    =\alpha_0 \alpha_1
    $$
    По формуле полной вероятности:
    $$
    P_{h_0\cap h_1}(A_0) = \sum_{\mathbf{z}} P_{h_0\cap h_1,\mathbf{Z=z}}(A_0) P_{h_0\cap h_1}(\mathbf{Z}=\mathbf{z})
    = \sum_{\mathbf{z}} \alpha_0 P_{h_0\cap h_1}(\mathbf{Z}=\mathbf{z})=\alpha_0
    $$
    Аналогично, $P_{h_0\cap h_1}(A_1)=\alpha_1$. Также по формуле полной вероятности:
    $$
    P_{h_0\cap h_1}(A_0 \cap A_1) = \sum_{\mathbf{z}} P_{h_0\cap h_1,\mathbf{Z=z}}(A_0 \cap A_1) P_{h_0\cap h_1}(\mathbf{Z}=\mathbf{z})=
    $$
    $$
    = \sum_{\mathbf{z}} \alpha_0 \alpha_1 P_{h_0\cap h_1}(\mathbf{Z}=\mathbf{z})
    = \alpha_0 \alpha_1
    $$
    Таким образом, $P_{h_0\cap h_1}(A_0 \cap A_1)=P_{h_0\cap h_1}(A_0) P_{h_0\cap h_1}(A_1)=\alpha_0 \alpha_1$.
\end{proof}
Применим теорему \ref{main_theorem} для проверки условной независимости в трехмерном
распределении Бернулли.
Положим индивидуальные гипотезы:
\begin{itemize}
    \item $h_0 : X$ и $Y$ независимы при условии $Z=0$
    \item $h_1 : X$ и $Y$ независимы при условии $Z=1$
\end{itemize}
Для проверки гипотез $h_0$ и $h_1$ будем использовать тесты
$\varphi_0 = \varphi^{\text{Independence}}_0$ и 
$\varphi_1 = \varphi^{\text{Independence}}_1$ уровня $\alpha_0$ и $\alpha_1$ 
по повторным выборкам
$\Xi_0$ и $\Xi_1$ соответственно.
Тогда гипотеза
условной независимости имеет вид $h = h_0 \cap h_1$ и тест проверки условной независимости можно определить как:
$$
\varphi^{\text{Subsamples}}=\begin{cases}
    1, \text{ наступило событие $A_0 \cup A_1$}\\
    0, \text{ иначе}
\end{cases}
$$
Пусть для простоты $\alpha_0 = \alpha_1 = \beta$.
Тогда $$P_{h_0 \cap h_1}(\varphi^{\text{Subsamples}}=1)=P_{h_0 \cap h_1}(A_0 \cup A_1)=$$ 
$$ = P_{h_0 \cap h_1}(A_0) + P_{h_0 \cap h_1}(A_1) - 
P_{h_0 \cap h_1}(A_0 \cap A_1) = 2\beta - \beta^2$$
Нетрудно проверить, что для контроля $P_{h_0 \cap h_1}(\varphi^{\text{Subsamples}}=1)=\alpha$ достаточно положить
уровень значимости $\beta = 1 - \sqrt{1-\alpha}$ на индивидуальных
тестах проверки гипотез $h_0$ и $h_1$.
